{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "from collections import Counter,defaultdict\n",
    "try:\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\tfrom nltk.corpus import stopwords\n",
    "\tfrom nltk.stem import PorterStemmer\n",
    "\timport gensim\n",
    "\tfrom gensim.models import Word2Vec\n",
    "\tfrom nltk.stem import WordNetLemmatizer\n",
    "\tfrom keras.utils import np_utils\n",
    "except:\n",
    "\tprint(\"require modules: keras,gensim,nltk.stem,nltk.corpus,nltk.stem, please install it.\")\n",
    "\texit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg1 = re.compile(\"\\.txt$\")\n",
    "reg2 = re.compile(\"([0-9]+)\\.txt\")\n",
    "reg3 = re.compile(\".*_([0-9])\\.txt\")\n",
    "reg4 = re.compile(\"\\[.+\\]\")\n",
    "reg5 = re.compile(\"info\\.txt\")\n",
    "lyrics_dic = {}\n",
    "#iter all directory and load all song(txt file)\n",
    "for i in os.listdir():\n",
    "    if os.path.isdir(i):\n",
    "        for path,sub,items in os.walk(i):\n",
    "            if any([reg1.findall(item) for item in items]):\n",
    "                for item in items:\n",
    "                    if reg5.findall(item):\n",
    "                        continue\n",
    "                    if reg3.findall(item):\n",
    "                        num = [\"0\"+reg3.findall(item)[0]]\n",
    "                        name = \"_\".join(path.split(\"/\") + num)\n",
    "                    else:\n",
    "                        name = \"_\".join(path.split(\"/\") + reg2.findall(item))\n",
    "\n",
    "                    with open(os.path.join(path,item),\"r\",encoding=\"utf8\", errors='ignore') as f:\n",
    "                        lyrics = \"\".join(f.readlines())\n",
    "                        lyrics = reg4.subn(\"\",lyrics)[0]\n",
    "                        lyrics_dic[name] = lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics_dic.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=[\"lyrics\",\"mood\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in lyrics_dic.keys():\n",
    "    #print(key)\n",
    "    if \"Happy\" in key:\n",
    "        \n",
    "        df=df.append({\"lyrics\":lyrics_dic[key],\"mood\":\"1\"},ignore_index=True)\n",
    "    elif \"Sad\" in key:\n",
    "        df=df.append({\"lyrics\":lyrics_dic[key],\"mood\":\"2\"},ignore_index=True)\n",
    "    elif \"Angry\" in key:\n",
    "        df=df.append({\"lyrics\":lyrics_dic[key],\"mood\":\"3\"},ignore_index=True)\n",
    "    elif \"Relaxed\" in key:\n",
    "        df=df.append({\"lyrics\":lyrics_dic[key],\"mood\":\"4\"},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Put your lips close to mine\\nAs long as they d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>My lullaby,hung out to dry\\nWhat's up with tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Though you've played at love and lost\\nAnd sor...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we know we are the ones\\nwho do it numb again\\...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Another day has come and gone\\nThey fade away ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics mood\n",
       "0  Put your lips close to mine\\nAs long as they d...    1\n",
       "1  My lullaby,hung out to dry\\nWhat's up with tha...    1\n",
       "2  Though you've played at love and lost\\nAnd sor...    1\n",
       "3  we know we are the ones\\nwho do it numb again\\...    1\n",
       "4  Another day has come and gone\\nThey fade away ...    1"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_csv(\"df_mood_backup.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>file</th>\n",
       "      <th>artist</th>\n",
       "      <th>title</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>tags</th>\n",
       "      <th>mood</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>9913</td>\n",
       "      <td>TRAREDZ12903CFB6E3.h5</td>\n",
       "      <td>Pelle Carlberg</td>\n",
       "      <td>Clever Girls Like Clever Boys Much More Than C...</td>\n",
       "      <td>You should have listened to what mama said\\nAn...</td>\n",
       "      <td>['handclaps', 'title is a full sentence', 'ind...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601</th>\n",
       "      <td>9943</td>\n",
       "      <td>TRATTMT128F149167B.h5</td>\n",
       "      <td>Michael Jackson</td>\n",
       "      <td>Ain't No Sunshine</td>\n",
       "      <td>Spoken Intro:\\nYou ever want something\\nThat y...</td>\n",
       "      <td>['soul', 'michael jackson', '70s', 'pop', 'cov...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>602</th>\n",
       "      <td>9966</td>\n",
       "      <td>TRAWFGF128E0792FE0.h5</td>\n",
       "      <td>Extreme</td>\n",
       "      <td>Stop The World</td>\n",
       "      <td>All the world's a masquerade\\nMade up of fools...</td>\n",
       "      <td>['hard rock', 'rock', 'funk metal', 'Power bal...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>603</th>\n",
       "      <td>9967</td>\n",
       "      <td>TRAWFVE128F42912CA.h5</td>\n",
       "      <td>Dimmu Borgir</td>\n",
       "      <td>Sympozium</td>\n",
       "      <td>Chains of despair cloacked by darkness\\nThe th...</td>\n",
       "      <td>['Symphonic Black Metal', 'black metal', 'melo...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>9969</td>\n",
       "      <td>TRBDMIN128F147FCBB.h5</td>\n",
       "      <td>Phil Collins</td>\n",
       "      <td>One More Night</td>\n",
       "      <td>One more night\\nOne more night\\n\\nI've been tr...</td>\n",
       "      <td>['80s', 'pop', 'Phil Collins', 'soft rock', 'r...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                   file           artist  \\\n",
       "600        9913  TRAREDZ12903CFB6E3.h5   Pelle Carlberg   \n",
       "601        9943  TRATTMT128F149167B.h5  Michael Jackson   \n",
       "602        9966  TRAWFGF128E0792FE0.h5          Extreme   \n",
       "603        9967  TRAWFVE128F42912CA.h5     Dimmu Borgir   \n",
       "604        9969  TRBDMIN128F147FCBB.h5     Phil Collins   \n",
       "\n",
       "                                                 title  \\\n",
       "600  Clever Girls Like Clever Boys Much More Than C...   \n",
       "601                                  Ain't No Sunshine   \n",
       "602                                     Stop The World   \n",
       "603                                          Sympozium   \n",
       "604                                     One More Night   \n",
       "\n",
       "                                                lyrics  \\\n",
       "600  You should have listened to what mama said\\nAn...   \n",
       "601  Spoken Intro:\\nYou ever want something\\nThat y...   \n",
       "602  All the world's a masquerade\\nMade up of fools...   \n",
       "603  Chains of despair cloacked by darkness\\nThe th...   \n",
       "604  One more night\\nOne more night\\n\\nI've been tr...   \n",
       "\n",
       "                                                  tags  mood  \n",
       "600  ['handclaps', 'title is a full sentence', 'ind...     1  \n",
       "601  ['soul', 'michael jackson', '70s', 'pop', 'cov...     2  \n",
       "602  ['hard rock', 'rock', 'funk metal', 'Power bal...     2  \n",
       "603  ['Symphonic Black Metal', 'black metal', 'melo...     3  \n",
       "604  ['80s', 'pop', 'Phil Collins', 'soft rock', 'r...     2  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df_new[[\"lyrics\",\"mood\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3=df.append(df_new,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"mood\"]=df3[\"mood\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1382, 2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df3[\"mood\"].values)\n",
    "df=df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_X = df['lyrics'].values\n",
    "data_Y = df['mood'].values\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "\n",
    "porter_stemmer = nltk.stem.porter.PorterStemmer()\n",
    "def porter_tokenizer(text, stemmer=porter_stemmer):\n",
    "    lower_txt = text.lower()\n",
    "    tokens = nltk.wordpunct_tokenize(lower_txt)\n",
    "    stems = [porter_stemmer.stem(t) for t in tokens]\n",
    "    no_punct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return no_punct\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='replace',\n",
    "            strip_accents='unicode',\n",
    "            analyzer='word',\n",
    "            binary=False,\n",
    "            stop_words=\"english\",\n",
    "            tokenizer=porter_tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data_X = tfidf.fit_transform(data_X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_X, data_Y, test_size=0.4, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[93 50  2 22]\n",
      " [46 98 15 37]\n",
      " [17 29 25  8]\n",
      " [21 56  3 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.53      0.56      0.54       167\n",
      "           2       0.42      0.50      0.46       196\n",
      "           3       0.56      0.32      0.40        79\n",
      "           4       0.32      0.28      0.30       111\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       553\n",
      "   macro avg       0.45      0.41      0.42       553\n",
      "weighted avg       0.45      0.45      0.44       553\n",
      "\n",
      "0.44665461121157324\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "import sklearn.metrics as metrics\n",
    "classifier_rbf = LinearSVC(multi_class=\"ovr\")\n",
    "classifier_rbf.fit(X_train, y_train)\n",
    "prediction_rbf = classifier_rbf.predict(X_test)\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "def prediction_evaluation(model_name,model,test_X,test_Y):\n",
    "    y_pred = model.predict(test_X)\n",
    "    cm = metrics.confusion_matrix(test_Y, y_pred)\n",
    "    print(cm)\n",
    "    print(classification_report(test_Y, y_pred))\n",
    "    print(accuracy_score(test_Y, y_pred))\n",
    "prediction_evaluation(\"SVM\",classifier_rbf,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_NB = MultinomialNB()\n",
    "clf_NB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 24 143   0   0]\n",
      " [  7 189   0   0]\n",
      " [  5  74   0   0]\n",
      " [  5 106   0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.14      0.23       167\n",
      "           2       0.37      0.96      0.53       196\n",
      "           3       0.00      0.00      0.00        79\n",
      "           4       0.00      0.00      0.00       111\n",
      "\n",
      "   micro avg       0.39      0.39      0.39       553\n",
      "   macro avg       0.24      0.28      0.19       553\n",
      "weighted avg       0.31      0.39      0.26       553\n",
      "\n",
      "0.38517179023508136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "prediction_evaluation(\"MultinomialNB\",clf_NB,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(500, 200), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf_mlp=MLPClassifier(hidden_layer_sizes=(500,200,), activation='relu', solver='adam', max_iter=1000,alpha=0.001)\n",
    "clf_mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 83  51   4  29]\n",
      " [ 43 110  11  32]\n",
      " [ 15  33  24   7]\n",
      " [ 24  63   1  23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.50      0.50       167\n",
      "           2       0.43      0.56      0.49       196\n",
      "           3       0.60      0.30      0.40        79\n",
      "           4       0.25      0.21      0.23       111\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       553\n",
      "   macro avg       0.45      0.39      0.40       553\n",
      "weighted avg       0.44      0.43      0.43       553\n",
      "\n",
      "0.43399638336347196\n"
     ]
    }
   ],
   "source": [
    "prediction_evaluation(\"MLPClassifier\",clf_mlp,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87  59   2  19]\n",
      " [ 45 119   7  25]\n",
      " [ 17  36  20   6]\n",
      " [ 21  65   1  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.52      0.52       167\n",
      "           2       0.43      0.61      0.50       196\n",
      "           3       0.67      0.25      0.37        79\n",
      "           4       0.32      0.22      0.26       111\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       553\n",
      "   macro avg       0.48      0.40      0.41       553\n",
      "weighted avg       0.47      0.45      0.44       553\n",
      "\n",
      "0.45207956600361665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf_lg=LogisticRegressionCV(max_iter=5000,multi_class=\"multinomial\")\n",
    "clf_lg.fit(X_train,y_train)\n",
    "prediction_evaluation(\"logistic\",clf_lg,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 47  65   3  12]\n",
      " [ 39 106  10  20]\n",
      " [  4  14   2   1]\n",
      " [ 16  33   1   8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.37      0.40       127\n",
      "           1       0.49      0.61      0.54       175\n",
      "           2       0.12      0.10      0.11        21\n",
      "           3       0.20      0.14      0.16        58\n",
      "\n",
      "   micro avg       0.43      0.43      0.43       381\n",
      "   macro avg       0.31      0.30      0.30       381\n",
      "weighted avg       0.41      0.43      0.41       381\n",
      "\n",
      "0.42782152230971127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "clf_lg=LogisticRegressionCV(max_iter=5000,multi_class=\"multinomial\")\n",
    "clf_lg.fit(xtrain_tfidf_ngram,train_y)\n",
    "prediction_evaluation(\"logistic\",clf_lg,xvalid_tfidf_ngram,valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k=\"now i cant live without you,i'm nothing without you.now i cant live without you, nothing without you.if i go away from you, be away from myself.because its only you,now its only you,you are my life now,peace in me and the pain too,now my love is just you. whats the relation between you and me!that i cant get away for a moment... i live for you everyday,i give every moment of my life to you.there's no moment without you,there's your name in every breath. because its only you,now its only you,you are my life now,peace in me and the pain too,You are my love,cause its only you,because its only you,now my love is just you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_k=tfidf.fit_transform([data_k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.2750095491084634\n",
      "  (0, 10)\t0.09166984970282113\n",
      "  (0, 12)\t0.18333969940564226\n",
      "  (0, 0)\t0.2750095491084634\n",
      "  (0, 1)\t0.2750095491084634\n",
      "  (0, 13)\t0.5500190982169268\n",
      "  (0, 7)\t0.2750095491084634\n",
      "  (0, 15)\t0.18333969940564226\n",
      "  (0, 14)\t0.18333969940564226\n",
      "  (0, 9)\t0.2750095491084634\n",
      "  (0, 6)\t0.18333969940564226\n",
      "  (0, 16)\t0.09166984970282113\n",
      "  (0, 11)\t0.2750095491084634\n",
      "  (0, 5)\t0.09166984970282113\n",
      "  (0, 4)\t0.18333969940564226\n",
      "  (0, 17)\t0.18333969940564226\n",
      "  (0, 2)\t0.09166984970282113\n",
      "  (0, 3)\t0.09166984970282113\n"
     ]
    }
   ],
   "source": [
    "print(data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 18 features per sample; expecting 6145",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-c9de7323c827>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf_lg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 262\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 18 features per sample; expecting 6145"
     ]
    }
   ],
   "source": [
    "clf_lg.predict(data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x6145 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 69 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_m = df['lyrics'].values\n",
    "data_m=np.append(data_m,data_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=tfidf.fit_transform(data_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(778, 6145)\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_rbf.predict(z[777])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/manojitpc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/manojitpc/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lyrics'] = [entry.lower() for entry in df['lyrics']]\n",
    "df['lyrics']= [word_tokenize(entry) for entry in df['lyrics']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(df['lyrics']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    df.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/manojitpc/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1382, 3)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = df['text_final'][0:1000]\n",
    "valid_x = df['text_final'][1001:1382]\n",
    "train_y = df['mood'][0:1000]\n",
    "valid_y = df['mood'][1001:1382]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "train_y = Encoder.fit_transform(train_y)\n",
    "valid_y = Encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "\n",
    "#Appending Training Set Texts\n",
    "for items in train_x:\n",
    "    all_texts.append(items)\n",
    "    \n",
    "#Appending Validation Set Texts\n",
    "for items in valid_x:\n",
    "    all_texts.append(items)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K    100% |████████████████████████████████| 645kB 2.4MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/site-packages (from textblob) (3.4.1)\n",
      "Requirement already satisfied: six in /Users/manojitpc/Library/Python/3.7/lib/python/site-packages (from nltk>=3.1->textblob) (1.11.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "import pandas,numpy, textblob, string\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import layers, models, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word')\n",
    "count_vect.fit(all_texts)\n",
    "\n",
    "# transform the training and validation data using count vectorizer object\n",
    "xtrain_count =  count_vect.transform(train_x)\n",
    "xvalid_count =  count_vect.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word',max_features=5000)\n",
    "tfidf_vect.fit(all_texts)\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', ngram_range=(2,2), max_features=5000)\n",
    "tfidf_vect_ngram.fit(all_texts)\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB, Count Vectors:  0.48293963254593175\n",
      "NB, WordLevel TF-IDF:  0.5301837270341208\n",
      "NB, N-Gram Vectors:  0.4776902887139108\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Count Vectors\n",
    "accuracy_count_nb = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"NB, Count Vectors: \", accuracy_count_nb)\n",
    "\n",
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy_word_nb = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy_word_nb)\n",
    "\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "accuracy_ngram_nb = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"NB, N-Gram Vectors: \", accuracy_ngram_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, Count Vectors:  0.5144356955380578\n",
      "RF, WordLevel TF-IDF Vectors:  0.5144356955380578\n",
      "RF, N-Gram Vectors:  0.3753280839895013\n"
     ]
    }
   ],
   "source": [
    "#Random Forest on Count Vectors\n",
    "accuracy_count_rf = train_model(RandomForestClassifier(n_estimators=100),xtrain_count,train_y,xvalid_count)\n",
    "print(\"RF, Count Vectors: \",accuracy_count_rf)\n",
    "\n",
    "#Random Forest on Word Level TF IDF Vectors\n",
    "accuracy_word_rf = train_model(RandomForestClassifier(n_estimators=100),xtrain_tfidf,train_y,xvalid_tfidf)\n",
    "print(\"RF, WordLevel TF-IDF Vectors: \",accuracy_word_rf)\n",
    "\n",
    "#Random Forest Ngram Level TF IDF Vectors\n",
    "accuracy_ngram_rf = train_model(RandomForestClassifier(n_estimators=100),xtrain_tfidf_ngram,train_y,xvalid_tfidf_ngram)\n",
    "print(\"RF, N-Gram Vectors: \",accuracy_ngram_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR, Count Vectors:  0.3910761154855643\n",
      "LR, WordLevel TF-IDF:  0.5091863517060368\n",
      "LR, N-Gram Vectors:  0.48293963254593175\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Count Vectors\n",
    "accuracy_count_lc = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "print (\"LR, Count Vectors: \", accuracy_count_lc)\n",
    "\n",
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy_word_lc = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "print (\"LR, WordLevel TF-IDF: \", accuracy_word_lc)\n",
    "\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy_ngram_lc = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print (\"LR, N-Gram Vectors: \", accuracy_ngram_lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
